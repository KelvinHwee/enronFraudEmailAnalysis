{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503e0bbd",
   "metadata": {},
   "source": [
    "###   Background of code\n",
    "\n",
    "\n",
    "We attempt to discover or mine information from the email dataset in different ways / visualization methods.\n",
    "\n",
    "**For this Jupyter Notebook, all graphics are interactive. But because of the challenge in rendering interactive graphics in Github, the visualisations are not shown. It is recommended that you download and run the Jupyter Notebook at your own time :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0f1d2",
   "metadata": {},
   "source": [
    "###   Basic configuration steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec592ea3",
   "metadata": {},
   "source": [
    "### We install some packages\n",
    "Packages for basic processing, data manipulation, visualisation and graphing of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1abeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import basic python packages\n",
    "import warnings\n",
    "import tkinter  # to show plot in Pycharm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# - import packages for data manipulations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# - import packages for visualisation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"svg\"\n",
    "\n",
    "# pio.renderers.default = \"browser\"\n",
    "# from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c7dcf",
   "metadata": {},
   "source": [
    "### Additionally, we install packages for NLP\n",
    "\n",
    "For the installation of \"en_core_web_sm\", we use the following command in terminal \"pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0.tar.gz\". The file was then downloaded to this folder: \"/home/kelvinhwee/.cache/pip/wheels/62/79/40/648305f0a2cd1fdab236bd6764ba467437c5fae2a925768153\"\n",
    "(look out for the installation completion message in the terminal).\n",
    "\n",
    "Lastly, we copied the zipped file, and extracted the \"en_core_web_sm-3.1.0\" folder (containing the \"config.cfg\" file) into the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d06b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kelvinhwee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # uncomment this if you run into punkt download issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7a3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we load the spacy trained pipelines (for English); this is an English pipeline optimized for CPU\n",
    "nlp = spacy.load('en_core_web_sm-3.1.0')\n",
    "\n",
    "# - initialise the spacy Matcher with a vocab; matcher must always share the same vocab with the documents it operate on\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b8b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - other configurations\n",
    "pd.set_option(\"display.max_column\", None)\n",
    "source_filepath = '/home/kelvinhwee/PycharmProjects/sourceFiles'\n",
    "\n",
    "# - packages created\n",
    "from utils import extract_domain, reformat_email_func\n",
    "from utils import one_to_one_mapping\n",
    "from utils import get_relation, get_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7efcd7",
   "metadata": {},
   "source": [
    "### Read CSV data file\n",
    "\n",
    "The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.\n",
    "\n",
    "This is the May 7, 2015 Version of dataset, as published at https://www.cs.cmu.edu/~./enron/. we note that there are only two columes, \"file\" and \"message\".\n",
    "\n",
    "For this exercise, we have used a smaller sample set which we derived from the full data (commands to load full data have been commented out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8e349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We look at a sample of the data: \n",
      "    Unnamed: 0                              file  \\\n",
      "0      237243          kean-s/attachments/1757.   \n",
      "1       63803    dasovich-j/all_documents/4101.   \n",
      "2      320451                 mann-k/sent/3197.   \n",
      "3      367550             quigley-d/fin_desk/6.   \n",
      "4       83252              dasovich-j/sent/700.   \n",
      "5      476834      taylor-m/all_documents/7964.   \n",
      "6      465472       symes-k/all_documents/3742.   \n",
      "7      112944       fischer-m/deleted_items/17.   \n",
      "8      107663  farmer-d/discussion_threads/553.   \n",
      "9      114674           forney-j/sent_items/95.   \n",
      "\n",
      "                                             message  \n",
      "0  Message-ID: <27407479.1075851045148.JavaMail.e...  \n",
      "1  Message-ID: <301250.1075843054571.JavaMail.eva...  \n",
      "2  Message-ID: <15348104.1075845996347.JavaMail.e...  \n",
      "3  Message-ID: <9943936.1075841449972.JavaMail.ev...  \n",
      "4  Message-ID: <32384912.1075843200467.JavaMail.e...  \n",
      "5  Message-ID: <23924850.1075860197196.JavaMail.e...  \n",
      "6  Message-ID: <13474213.1075841707824.JavaMail.e...  \n",
      "7  Message-ID: <20240491.1075853098146.JavaMail.e...  \n",
      "8  Message-ID: <31574274.1075854056946.JavaMail.e...  \n",
      "9  Message-ID: <12793475.1075852369729.JavaMail.e...  \n"
     ]
    }
   ],
   "source": [
    "# # - read in the CSV data\n",
    "# emails_df = pd.read_csv(source_filepath + '/emails.csv')\n",
    "#\n",
    "# - read in the sample CSV data\n",
    "# import random\n",
    "# sample_vals = random.sample(list(range(emails_df.shape[0])), 5000)\n",
    "# emails_df.loc[sample_vals].to_csv(source_filepath + '/sample_emails.csv')\n",
    "\n",
    "emails_df = pd.read_csv(source_filepath + '/sample_emails.csv')\n",
    "print(\"We look at a sample of the data: \\n\", emails_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ba0c9",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Extract critical data points from email messages. For a start, we try to replace some characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca987957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(emails_df.shape[0]):\n",
    "    temp_text = emails_df[\"message\"][i]\n",
    "    new_text = temp_text.replace(\"\\n \", \" \")  # some \"\\n \" in subject; , clean them to space character\n",
    "    # new_text = new_text.replace(\"\\n\\n\", \"\\n\")  # dropped this; the one after \"filename\" always has double \"\\n\"\n",
    "    new_text = new_text.replace(\"Re: \", \"\")  # some \"Re: \" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"Fw: \", \"\")  # some \"Fw: \" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"\\n\\t\", \"\")  # very long recipient list has \"\\n\\t\"; clean them to blanks\n",
    "    new_text = new_text.replace(\" : \", \"\")  # some \":\" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"[IMAGE]\", \"\")  # some \"[IMAGE]\" tags; , clean them to blanks\n",
    "    emails_df.loc[i, \"message\"] = new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1c355",
   "metadata": {},
   "source": [
    "Next we try to collate the list of fields present in the email message and then we create a dictionary object to do some information extraction for processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b733c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we collate the list of \"keys\"\n",
    "keys_list = ['Message-ID', 'Date', 'From', 'To', 'Subject', 'Cc', 'Mime-Version', 'Content-Type',\n",
    "             'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin',\n",
    "             'X-FileName']\n",
    "\n",
    "fields_list = keys_list + [\"Sent\"]  # to add in additional fields to clean (for RegEx later)\n",
    "fields_list_plus = fields_list + [i.lower() for i in fields_list] \\\n",
    "                   + [i.upper() for i in fields_list]  # include variations of lower and upper case\n",
    "\n",
    "# - create dictionary (using dictionary comprehension) to do \"conversion\" later on (you will see)\n",
    "keys_dict = {i: [k, len(k)] for i, k in enumerate(keys_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5038561",
   "metadata": {},
   "source": [
    "We try to perform a batch-wise extraction of the email contents based on the placeholders e.g. \"To\", \"From\", \"Subject\". We collate the list of regex logic first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a265184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex logic\n",
    "clean_html_tags = re.compile(\"<[/]*.*?>|&nbsp;\")\n",
    "clean_multi_space = re.compile(\"[\\s]{2,}\")\n",
    "clean_field_headers = re.compile('|'.join([item + \":\" for item in fields_list_plus]))\n",
    "clean_emails = re.compile(\"[\\w._]+@[\\w.]+\")\n",
    "clean_fwds = re.compile(\"[-_]{2,}.*?[-_]{2,}|FW:|Fwd:|RE:\")  # cleans \"Forwarded by\" in between long dashes and others\n",
    "clean_unintended_sends = re.compile(\"[-_*]{2,}.*?[-_*]{2,}\")\n",
    "clean_dashes = re.compile(\"[-]{2,}\")\n",
    "clean_transmission_warn = re.compile(r\"The information.*?any computer.\")  # cleans warning texts\n",
    "clean_datetime = re.compile(\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\\s+[\\d]{1,2}:[\\d]{1,2}[:\\d]*\\s+[AMPM]+\")  # for format DD/MM/YYYY XX:XX:XX AM/PM\n",
    "clean_multi_symbols = re.compile(\"[>,(\\\"\\'\\\\!.\\[\\]-]+\\s?[>,(\\\"\\'\\\\!.\\[\\]-]+\")  # e.g. \"> >\", \", , \", \", (\"\n",
    "clean_addr_code = re.compile(\"[, ]*[A-Z]{2}\\s+[\\d]{5}\")  # cleans \", TX 77082\"\n",
    "clean_phone_fax = re.compile(\"[\\d]*[-]*[\\d]{3}-[\\d]{3}-[\\d]{4}[\\s]*[(]*\\w*[)]*\")  # \"713-853-3989 (Phone)\", \"713-646-3393(Fax\", \"1-888-334-4204\"\n",
    "clean_phone_ctrycode = re.compile(\"\\([\\d]{3}\\)[\\s]*[\\d]{3}-[\\d]{4}\")  # (281) 558-9198, (713) 670-2457\n",
    "clean_link = re.compile(r\"[http]*[https]*[:/]*/?[\\w]+[.][\\w]+.*[.][\\w]+\")  # e.g. http://explorer.msn.com, https://explorer.msn.com.net\"\n",
    "clean_email_codes = re.compile(\"[=][\\d]+\")  # clear email codes \"=19\", \"=20\"\n",
    "clean_very_long_text = re.compile(\"[\\w+]{20,}\")\n",
    "# Other things to clean: Staff Meeting - Mt. Ranier 5/30/2001 Time: 1:00 PM - 3:00 PM (Central Standard Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05b5e8",
   "metadata": {},
   "source": [
    "We now perform the batch-wise extraction of email contents using the regex logic compiled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3f2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dict = []\n",
    "for i in range(emails_df.shape[0]):  # i=4\n",
    "\n",
    "    email_dict = {}  # empty dictionary to store the key-value pair\n",
    "    temp_str = emails_df[\"message\"][i].split(\"\\n\")  # assign string to variable; so can insert values to specific place\n",
    "\n",
    "    # this step uses the above created dictionary to \"impute\" keys if there are missing key values, e.g. \"To\", \"Cc\"\n",
    "    for pos in range(len(keys_list)):\n",
    "        if temp_str[pos][0:len(keys_dict[pos][0])] != keys_dict[pos][0]:\n",
    "            temp_str.insert(pos, str(keys_dict[pos][0]) + ': ')\n",
    "\n",
    "    # this step performs the split and extract the key-value pair for the standard known field headers\n",
    "    for j in range(0, 17):\n",
    "        key = temp_str[j].split(\":\")[0]\n",
    "        val = ':'.join(temp_str[j].split(\":\")[1:]).strip()\n",
    "        email_dict[key] = val\n",
    "\n",
    "    # this step saves the body of the text; we apply some regex logic\n",
    "    text_body = temp_str[17:]\n",
    "    text_body = \" \".join([text for text in text_body]).strip()  # joins back all elements into a single string\n",
    "\n",
    "    # apply regex logic\n",
    "    text_body = re.sub(clean_field_headers, \"\", text_body)\n",
    "    text_body = re.sub(clean_html_tags, \"\", text_body)\n",
    "    text_body = re.sub(clean_emails, \"\", text_body)\n",
    "    text_body = re.sub(clean_fwds, \" \", text_body)\n",
    "    text_body = re.sub(clean_unintended_sends, \" \", text_body)\n",
    "    text_body = re.sub(clean_dashes, \" \", text_body)\n",
    "    text_body = re.sub(clean_transmission_warn, \" \", text_body)\n",
    "    text_body = re.sub(clean_datetime, \" \", text_body)\n",
    "    text_body = re.sub(clean_link, \" \", text_body)\n",
    "    text_body = re.sub(clean_phone_fax, \" \", text_body)\n",
    "    text_body = re.sub(clean_phone_ctrycode, \" \", text_body)\n",
    "    text_body = re.sub(clean_addr_code, \" \", text_body)\n",
    "    text_body = re.sub(clean_email_codes, \"\", text_body)\n",
    "    text_body = re.sub(clean_very_long_text, \"\", text_body)\n",
    "    text_body = re.sub(clean_multi_symbols, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_space, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_symbols, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_space, \" \", text_body)\n",
    "\n",
    "    # this step saves the email body text as a value to the key named \"body\"\n",
    "    email_dict[\"body\"] = text_body.strip()\n",
    "\n",
    "    # append the dictionary to a list, and later store as a dataframe\n",
    "    list_of_dict.append(email_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99684f",
   "metadata": {},
   "source": [
    "We compile the dictionary into a dataframe and renamed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "370bfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df_feat = pd.DataFrame(list_of_dict)\n",
    "emails_df_feat.columns = ['Message-ID', 'DateTime', 'From', 'To', 'Subject', 'Cc', 'Mime-Version',  # Date -> DateTime\n",
    "                          'Content-Type', 'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To',\n",
    "                          'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName', 'body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8d9a9",
   "metadata": {},
   "source": [
    "We further clean up the email addresses in the \"From\". \"To\", \"Cc\", \"Bcc\" fields using Regex groups. E.g \"houston <.ward@enron.com>\", \"e-mail <.brandon@enron.com>\"; unlike the usual \"houston.ward@enron.com\".\n",
    "\n",
    "We then replace the columns with the cleaned up emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4528b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we do a clean up of email addresses\n",
    "reformat_emails = re.compile(r\"(?P<part1>[\\w-]+)[<\\s]*(?P<part2>[\\w.\\'\\W]+)(?P<domain>[@\\w.-]+)\")\n",
    "\n",
    "cleaned_from_emails = reformat_email_func(emails_df_feat, \"From\", reformat_emails)\n",
    "cleaned_to_emails = reformat_email_func(emails_df_feat, \"To\", reformat_emails)\n",
    "cleaned_cc_emails = reformat_email_func(emails_df_feat, \"Cc\", reformat_emails)\n",
    "cleaned_bcc_emails = reformat_email_func(emails_df_feat, \"Bcc\", reformat_emails)\n",
    "\n",
    "# - replace the columns with the cleaned up emails\n",
    "emails_df_feat.From = cleaned_from_emails\n",
    "emails_df_feat.To = cleaned_to_emails\n",
    "emails_df_feat.Cc = cleaned_cc_emails\n",
    "emails_df_feat.Bcc = cleaned_bcc_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14f96c",
   "metadata": {},
   "source": [
    "We create new columns to include reformatted data: date, time, domain name (From and To) for emails.\n",
    "\n",
    "It seems like \"strptime\" cannot handle Timezone codes directly and more steps are required. We are not going to need Timezone information for our purpose, so we are going to ignore them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199d0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df_feat[\"date\"] = emails_df_feat[\"DateTime\"].apply(lambda x: datetime\n",
    "                                                          .strptime(x[:-6], \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "                                                          .strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "emails_df_feat[\"time\"] = emails_df_feat[\"DateTime\"].apply(lambda x: datetime\n",
    "                                                          .strptime(x[:-6], \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "                                                          .strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "emails_df_feat[\"From_domain\"] = extract_domain(emails_df_feat, \"From\")\n",
    "emails_df_feat[\"To_domain\"] = extract_domain(emails_df_feat, \"To\")\n",
    "emails_df_feat[\"Cc_domain\"] = extract_domain(emails_df_feat, \"Cc\")\n",
    "emails_df_feat[\"Bcc_domain\"] = extract_domain(emails_df_feat, \"Bcc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb731df",
   "metadata": {},
   "source": [
    "### Quick glance at the resultant new dataframe (post-feature engineering)\n",
    "\n",
    "We take a quick look at some of the rows based on some sample name matches; we used \"apply\" together with \"str\" and \"contains\", and together with \"loc\", we can extract rows based on columns that have lists as elements https://www.chicagotribune.com/sns-ap-enron-trial-glance-story.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0370b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Message-ID  \\\n",
      "24  <21723779.1075857674036.JavaMail.evans@thyme>   \n",
      "40  <31890614.1075845545604.JavaMail.evans@thyme>   \n",
      "54  <13370742.1075851656808.JavaMail.evans@thyme>   \n",
      "71  <15840911.1075852366378.JavaMail.evans@thyme>   \n",
      "91  <25943015.1075858769769.JavaMail.evans@thyme>   \n",
      "\n",
      "                                 DateTime                       From  \\\n",
      "24   Fri, 8 Jun 2001 05:52:33 -0700 (PDT)          [black@enron.com]   \n",
      "40   Tue, 2 Jan 2001 07:38:00 -0800 (PST)  [john.lavorato@enron.com]   \n",
      "54  Fri, 12 Oct 2001 08:03:27 -0700 (PDT)    [ray.alvarez@enron.com]   \n",
      "71  Mon, 22 Oct 2001 08:22:03 -0700 (PDT)      [l..denton@enron.com]   \n",
      "91  Fri, 28 Sep 2001 19:05:56 -0700 (PDT)     [fran.chang@enron.com]   \n",
      "\n",
      "                                                   To  \\\n",
      "24  [tim.belden@enron.com, john.lavorato@enron.com...   \n",
      "40                         [david.delainey@enron.com]   \n",
      "54  [l..nicolay@enron.com, alan.comnes@enron.com, ...   \n",
      "71  [andrea.dahlke@enron.com, casey.evans@enron.co...   \n",
      "91  [tom.alonso@enron.com, tim.belden@enron.com, m...   \n",
      "\n",
      "                               Subject  \\\n",
      "24                        news stories   \n",
      "40                                       \n",
      "54  Operational Audit of CAISO by FERC   \n",
      "71          CNC Containers Corporation   \n",
      "91         09-28-01 West Power Reports   \n",
      "\n",
      "                                                   Cc Mime-Version  \\\n",
      "24                                                 []          1.0   \n",
      "40                                                 []          1.0   \n",
      "54                        [richard.shapiro@enron.com]          1.0   \n",
      "71                                                 []          1.0   \n",
      "91  [john.postlethwaite@enron.com, samantha.law@en...          1.0   \n",
      "\n",
      "                    Content-Type Content-Transfer-Encoding  \\\n",
      "24  text/plain; charset=us-ascii                      7bit   \n",
      "40  text/plain; charset=us-ascii                      7bit   \n",
      "54  text/plain; charset=us-ascii                      7bit   \n",
      "71  text/plain; charset=us-ascii                      7bit   \n",
      "91  text/plain; charset=us-ascii                      7bit   \n",
      "\n",
      "                                                  Bcc  \\\n",
      "24                                                 []   \n",
      "40                                                 []   \n",
      "54                        [richard.shapiro@enron.com]   \n",
      "71                                                 []   \n",
      "91  [john.postlethwaite@enron.com, samantha.law@en...   \n",
      "\n",
      "                                               X-From  \\\n",
      "24                                         Black, Don   \n",
      "40                                    John J Lavorato   \n",
      "54  Alvarez, Ray </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...   \n",
      "71  Denton, Rhonda L. </O=ENRON/OU=NA/CN=RECIPIENT...   \n",
      "91  Chang, Fran </O=ENRON/OU=NA/CN=RECIPIENTS/CN=F...   \n",
      "\n",
      "                                                 X-To  \\\n",
      "24  Belden, Tim </o=ENRON/ou=NA/cn=Recipients/cn=T...   \n",
      "40                                   David W Delainey   \n",
      "54  Nicolay, Christi L. </O=ENRON/OU=NA/CN=RECIPIE...   \n",
      "71  Dahlke, Andrea </O=ENRON/OU=NA/CN=RECIPIENTS/C...   \n",
      "91  Alonso, Tom </O=ENRON/OU=NA/CN=RECIPIENTS/CN=T...   \n",
      "\n",
      "                                                 X-cc X-bcc  \\\n",
      "24                                                            \n",
      "40                                                            \n",
      "54  Shapiro, Richard </O=ENRON/OU=NA/CN=RECIPIENTS...         \n",
      "71                                                            \n",
      "91  Postlethwaite, John </O=ENRON/OU=NA/CN=RECIPIE...         \n",
      "\n",
      "                                             X-Folder    X-Origin  \\\n",
      "24                             \\jlavora\\Deleted Items  Lavorado-J   \n",
      "40  \\John_Lavorato_Oct2001\\Notes Folders\\All docum...  LAVORATO-J   \n",
      "54  \\Dasovich, Jeff (Non-Privileged)\\Dasovich, Jef...  DASOVICH-J   \n",
      "71  \\JFORNEY (Non-Privileged)\\Forney, John M.\\Dele...    FORNEY-J   \n",
      "91             \\SWHITE (Non-Privileged)\\Deleted Items     White-S   \n",
      "\n",
      "                             X-FileName  \\\n",
      "24                          jlavora.pst   \n",
      "40                          jlavora.nsf   \n",
      "54  Dasovich, Jeff (Non-Privileged).pst   \n",
      "71         JFORNEY (Non-Privileged).pst   \n",
      "91          SWHITE (Non-Privileged).pst   \n",
      "\n",
      "                                                 body        date      time  \\\n",
      "24  Peggy Mahoney David W Delainey/HOU/, Janet R D...  2001-06-08  05:52:33   \n",
      "40    Do we need to talk about VP and MD nominations.  2001-01-02  07:38:00   \n",
      "54  Here are some bullets briefly describing the F...  2001-10-12  08:03:27   \n",
      "71  We received the executed EEI Master Power Purc...  2001-10-22  08:22:03   \n",
      "91  Canada's P&L will be updated on Monday (10/1) ...  2001-09-28  19:05:56   \n",
      "\n",
      "   From_domain To_domain Cc_domain Bcc_domain  \n",
      "24     [enron]   [enron]        []         []  \n",
      "40     [enron]   [enron]        []         []  \n",
      "54     [enron]   [enron]   [enron]    [enron]  \n",
      "71     [enron]   [enron]        []         []  \n",
      "91     [enron]   [enron]   [enron]    [enron]  \n"
     ]
    }
   ],
   "source": [
    "convicted_names = ['kenneth.lay','jeffrey.skilling','kevin.howard','michael.krautz','joe.hirko','rex.shelby',\n",
    "                   'scott.yeager','andrew.fastow','david.bermingham','giles.darby','gary.mulgrew','daniel.bayly',\n",
    "                   'james.brown','robert.furst','william.fuhs','dan.boyle','sheila.kahanek','christopher.calger',\n",
    "                   'richard.causey','lfastow','paula.rieker','ken.rice','mark.koenig','kevin.hannon','tim.despain',\n",
    "                   'jeff.richter','ben.glisan','david.delainey','michael.kopper','tim.belden','larry.lawyer',\n",
    "                   'david.duncan','wes.colwell','raymond.bowen']\n",
    "\n",
    "print(emails_df_feat.loc[(emails_df_feat.To.apply(lambda x : str(x)).str.contains('|'.join(convicted_names))), :].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17400301",
   "metadata": {},
   "source": [
    "### Find relationships using Sankey diagram\n",
    "\n",
    "We want to spot associations based on email address domains rather than names. The following part will prepare the data for the Sankey diagram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0ddc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create the dataframe that will contain the \"source\" and \"destination\"\n",
    "source_domains = emails_df_feat.From_domain.to_list()\n",
    "dest_domains = [list(set(emails_df_feat.loc[num, \"To_domain\"])) for num in\n",
    "                range(emails_df_feat.shape[0])]  # de-duplicated\n",
    "\n",
    "# - introduce the source and destination lists and the dataframe with all the one-to-one mapping will be done\n",
    "sankey_source, sankey_destin, source_dest_map_dedup, counter = one_to_one_mapping(source_domains, dest_domains)\n",
    "\n",
    "# - obtain a dictionary to map the domain names into indices for purpose of plotting Sankey diagram\n",
    "full_list_of_domains = sorted(list(set(sankey_source + sankey_destin)))\n",
    "domain_dict = {domain: num for num, domain in enumerate(full_list_of_domains)}  # dictionary comprehension on key-value\n",
    "\n",
    "# - create the fields required for the Sankey diagram\n",
    "s2 = []  # to be directly used in plotting the Sankey diagram\n",
    "d2 = []  # to be directly used in plotting the Sankey diagram\n",
    "v2 = []  # to be directly used in plotting the Sankey diagram\n",
    "for i in range(len(source_dest_map_dedup)):\n",
    "    tag1 = source_dest_map_dedup[i]\n",
    "    s2.append(domain_dict[tag1[0]])\n",
    "    d2.append(domain_dict[tag1[1]])\n",
    "    v2.append(counter[tag1[0], tag1[1]])\n",
    "\n",
    "# - plot the sankey diagram\n",
    "random.seed(24)\n",
    "sample_vals = random.sample(range(0, len(s2)), 120)  # we sample 120 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7726515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cafee03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27691/1575640975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfig1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Sankey Diagram to show associations based on domains\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfig1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/enronFraudEmailAnalysis/venv/lib/python3.9/site-packages/plotly/basedatatypes.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3396\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/enronFraudEmailAnalysis/venv/lib/python3.9/site-packages/plotly/io/_renderers.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Mimetype renderers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mbundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_mime_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderers_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mipython_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/enronFraudEmailAnalysis/venv/lib/python3.9/site-packages/plotly/io/_renderers.py\u001b[0m in \u001b[0;36m_build_mime_bundle\u001b[0;34m(self, fig_dict, renderers_string, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0mbundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_mimebundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbundle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/enronFraudEmailAnalysis/venv/lib/python3.9/site-packages/plotly/io/_base_renderers.py\u001b[0m in \u001b[0;36mto_mimebundle\u001b[0;34m(self, fig_dict)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_mimebundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         image_bytes = to_image(\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/enronFraudEmailAnalysis/venv/lib/python3.9/site-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Raise informative error message if Kaleido is not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    135\u001b[0m             \"\"\"\n\u001b[1;32m    136\u001b[0m \u001b[0mImage\u001b[0m \u001b[0mexport\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m\"kaleido\"\u001b[0m \u001b[0mengine\u001b[0m \u001b[0mrequires\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkaleido\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "fig1 = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 5, thickness = 20, line = dict(color = \"black\", width = 0.5),\n",
    "        label = full_list_of_domains, color = \"#3944BC\"\n",
    "    ),\n",
    "    link = dict(source = [s2[i] for i in sample_vals],\n",
    "                target = [d2[i] for i in sample_vals],\n",
    "                value = [v2[i] for i in sample_vals],\n",
    "                color = \"#F699CF\"))])\n",
    "\n",
    "fig1.update_layout(title_text=\"Sankey Diagram to show associations based on domains\", font_size=10)\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4e0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7836ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa70d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae347c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aac29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bc1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa970c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fcfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fe184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074c88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e0345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65747a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fdae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d88081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c2a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0320f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999b059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a1428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39029167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c50670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a339ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d481b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be79679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564c9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
