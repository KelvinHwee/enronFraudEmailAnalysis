{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503e0bbd",
   "metadata": {},
   "source": [
    "###   Background of code\n",
    "\n",
    "\n",
    "We attempt to discover or mine information from the email dataset in different ways / visualization methods.\n",
    "\n",
    "**For this Jupyter Notebook, all graphics are interactive. But because of the challenge in rendering interactive graphics in Github, the visualisations are not shown. It is recommended that you download and run the Jupyter Notebook at your own time :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0f1d2",
   "metadata": {},
   "source": [
    "###   Basic configuration steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec592ea3",
   "metadata": {},
   "source": [
    "### We install some packages\n",
    "Packages for basic processing, data manipulation, visualisation and graphing of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1abeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import basic python packages\n",
    "import warnings\n",
    "import tkinter  # to show plot in Pycharm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# - import packages for data manipulations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# - import packages for visualisation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "# pio.renderers.default = \"svg\"\n",
    "pio.renderers.default = \"browser\"\n",
    "\n",
    "\n",
    "# from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c7dcf",
   "metadata": {},
   "source": [
    "### Additionally, we install packages for NLP\n",
    "\n",
    "For the installation of \"en_core_web_sm\", we use the following command in terminal \"pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0.tar.gz\". The file was then downloaded to this folder: \"/home/kelvinhwee/.cache/pip/wheels/62/79/40/648305f0a2cd1fdab236bd6764ba467437c5fae2a925768153\"\n",
    "(look out for the installation completion message in the terminal).\n",
    "\n",
    "Lastly, we copied the zipped file, and extracted the \"en_core_web_sm-3.1.0\" folder (containing the \"config.cfg\" file) into the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d06b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kelvinhwee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # uncomment this if you run into punkt download issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7a3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we load the spacy trained pipelines (for English); this is an English pipeline optimized for CPU\n",
    "nlp = spacy.load('en_core_web_sm-3.1.0')\n",
    "\n",
    "# - initialise the spacy Matcher with a vocab; matcher must always share the same vocab with the documents it operate on\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b8b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - other configurations\n",
    "pd.set_option(\"display.max_column\", None)\n",
    "source_filepath = '/home/kelvinhwee/PycharmProjects/sourceFiles'\n",
    "\n",
    "# - packages created\n",
    "from utils import extract_domain, reformat_email_func\n",
    "from utils import one_to_one_mapping\n",
    "from utils import get_relation, get_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7efcd7",
   "metadata": {},
   "source": [
    "### Read CSV data file\n",
    "\n",
    "The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.\n",
    "\n",
    "This is the May 7, 2015 Version of dataset, as published at https://www.cs.cmu.edu/~./enron/. we note that there are only two columes, \"file\" and \"message\".\n",
    "\n",
    "For this exercise, we have used a smaller sample set which we derived from the full data (commands to load full data have been commented out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8e349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We look at a sample of the data: \n",
      "    Unnamed: 0                              file  \\\n",
      "0      237243          kean-s/attachments/1757.   \n",
      "1       63803    dasovich-j/all_documents/4101.   \n",
      "2      320451                 mann-k/sent/3197.   \n",
      "3      367550             quigley-d/fin_desk/6.   \n",
      "4       83252              dasovich-j/sent/700.   \n",
      "5      476834      taylor-m/all_documents/7964.   \n",
      "6      465472       symes-k/all_documents/3742.   \n",
      "7      112944       fischer-m/deleted_items/17.   \n",
      "8      107663  farmer-d/discussion_threads/553.   \n",
      "9      114674           forney-j/sent_items/95.   \n",
      "\n",
      "                                             message  \n",
      "0  Message-ID: <27407479.1075851045148.JavaMail.e...  \n",
      "1  Message-ID: <301250.1075843054571.JavaMail.eva...  \n",
      "2  Message-ID: <15348104.1075845996347.JavaMail.e...  \n",
      "3  Message-ID: <9943936.1075841449972.JavaMail.ev...  \n",
      "4  Message-ID: <32384912.1075843200467.JavaMail.e...  \n",
      "5  Message-ID: <23924850.1075860197196.JavaMail.e...  \n",
      "6  Message-ID: <13474213.1075841707824.JavaMail.e...  \n",
      "7  Message-ID: <20240491.1075853098146.JavaMail.e...  \n",
      "8  Message-ID: <31574274.1075854056946.JavaMail.e...  \n",
      "9  Message-ID: <12793475.1075852369729.JavaMail.e...  \n"
     ]
    }
   ],
   "source": [
    "# # - read in the CSV data\n",
    "# emails_df = pd.read_csv(source_filepath + '/emails.csv')\n",
    "#\n",
    "# - read in the sample CSV data\n",
    "# import random\n",
    "# sample_vals = random.sample(list(range(emails_df.shape[0])), 5000)\n",
    "# emails_df.loc[sample_vals].to_csv(source_filepath + '/sample_emails.csv')\n",
    "\n",
    "emails_df = pd.read_csv(source_filepath + '/sample_emails.csv')\n",
    "print(\"We look at a sample of the data: \\n\", emails_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ba0c9",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Extract critical data points from email messages. For a start, we try to replace some characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca987957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(emails_df.shape[0]):\n",
    "    temp_text = emails_df[\"message\"][i]\n",
    "    new_text = temp_text.replace(\"\\n \", \" \")  # some \"\\n \" in subject; , clean them to space character\n",
    "    # new_text = new_text.replace(\"\\n\\n\", \"\\n\")  # dropped this; the one after \"filename\" always has double \"\\n\"\n",
    "    new_text = new_text.replace(\"Re: \", \"\")  # some \"Re: \" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"Fw: \", \"\")  # some \"Fw: \" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"\\n\\t\", \"\")  # very long recipient list has \"\\n\\t\"; clean them to blanks\n",
    "    new_text = new_text.replace(\" : \", \"\")  # some \":\" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"[IMAGE]\", \"\")  # some \"[IMAGE]\" tags; , clean them to blanks\n",
    "    emails_df.loc[i, \"message\"] = new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1c355",
   "metadata": {},
   "source": [
    "Next we try to collate the list of fields present in the email message and then we create a dictionary object to do some information extraction for processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b733c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we collate the list of \"keys\"\n",
    "keys_list = ['Message-ID', 'Date', 'From', 'To', 'Subject', 'Cc', 'Mime-Version', 'Content-Type',\n",
    "             'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin',\n",
    "             'X-FileName']\n",
    "\n",
    "fields_list = keys_list + [\"Sent\"]  # to add in additional fields to clean (for RegEx later)\n",
    "fields_list_plus = fields_list + [i.lower() for i in fields_list] \\\n",
    "                   + [i.upper() for i in fields_list]  # include variations of lower and upper case\n",
    "\n",
    "# - create dictionary (using dictionary comprehension) to do \"conversion\" later on (you will see)\n",
    "keys_dict = {i: [k, len(k)] for i, k in enumerate(keys_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5038561",
   "metadata": {},
   "source": [
    "We try to perform a batch-wise extraction of the email contents based on the placeholders e.g. \"To\", \"From\", \"Subject\". We collate the list of regex logic first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a265184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex logic\n",
    "clean_html_tags = re.compile(\"<[/]*.*?>|&nbsp;\")\n",
    "clean_multi_space = re.compile(\"[\\s]{2,}\")\n",
    "clean_field_headers = re.compile('|'.join([item + \":\" for item in fields_list_plus]))\n",
    "clean_emails = re.compile(\"[\\w._]+@[\\w.]+\")\n",
    "clean_fwds = re.compile(\"[-_]{2,}.*?[-_]{2,}|FW:|Fwd:|RE:\")  # cleans \"Forwarded by\" in between long dashes and others\n",
    "clean_unintended_sends = re.compile(\"[-_*]{2,}.*?[-_*]{2,}\")\n",
    "clean_dashes = re.compile(\"[-]{2,}\")\n",
    "clean_transmission_warn = re.compile(r\"The information.*?any computer.\")  # cleans warning texts\n",
    "clean_datetime = re.compile(\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\\s+[\\d]{1,2}:[\\d]{1,2}[:\\d]*\\s+[AMPM]+\")  # for format DD/MM/YYYY XX:XX:XX AM/PM\n",
    "clean_multi_symbols = re.compile(\"[>,(\\\"\\'\\\\!.\\[\\]-]+\\s?[>,(\\\"\\'\\\\!.\\[\\]-]+\")  # e.g. \"> >\", \", , \", \", (\"\n",
    "clean_addr_code = re.compile(\"[, ]*[A-Z]{2}\\s+[\\d]{5}\")  # cleans \", TX 77082\"\n",
    "clean_phone_fax = re.compile(\"[\\d]*[-]*[\\d]{3}-[\\d]{3}-[\\d]{4}[\\s]*[(]*\\w*[)]*\")  # \"713-853-3989 (Phone)\", \"713-646-3393(Fax\", \"1-888-334-4204\"\n",
    "clean_phone_ctrycode = re.compile(\"\\([\\d]{3}\\)[\\s]*[\\d]{3}-[\\d]{4}\")  # (281) 558-9198, (713) 670-2457\n",
    "clean_link = re.compile(r\"[http]*[https]*[:/]*/?[\\w]+[.][\\w]+.*[.][\\w]+\")  # e.g. http://explorer.msn.com, https://explorer.msn.com.net\"\n",
    "clean_email_codes = re.compile(\"[=][\\d]+\")  # clear email codes \"=19\", \"=20\"\n",
    "clean_very_long_text = re.compile(\"[\\w+]{20,}\")\n",
    "# Other things to clean: Staff Meeting - Mt. Ranier 5/30/2001 Time: 1:00 PM - 3:00 PM (Central Standard Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05b5e8",
   "metadata": {},
   "source": [
    "We now perform the batch-wise extraction of email contents using the regex logic compiled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3f2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dict = []\n",
    "for i in range(emails_df.shape[0]):  # i=4\n",
    "\n",
    "    email_dict = {}  # empty dictionary to store the key-value pair\n",
    "    temp_str = emails_df[\"message\"][i].split(\"\\n\")  # assign string to variable; so can insert values to specific place\n",
    "\n",
    "    # this step uses the above created dictionary to \"impute\" keys if there are missing key values, e.g. \"To\", \"Cc\"\n",
    "    for pos in range(len(keys_list)):\n",
    "        if temp_str[pos][0:len(keys_dict[pos][0])] != keys_dict[pos][0]:\n",
    "            temp_str.insert(pos, str(keys_dict[pos][0]) + ': ')\n",
    "\n",
    "    # this step performs the split and extract the key-value pair for the standard known field headers\n",
    "    for j in range(0, 17):\n",
    "        key = temp_str[j].split(\":\")[0]\n",
    "        val = ':'.join(temp_str[j].split(\":\")[1:]).strip()\n",
    "        email_dict[key] = val\n",
    "\n",
    "    # this step saves the body of the text; we apply some regex logic\n",
    "    text_body = temp_str[17:]\n",
    "    text_body = \" \".join([text for text in text_body]).strip()  # joins back all elements into a single string\n",
    "\n",
    "    # apply regex logic\n",
    "    text_body = re.sub(clean_field_headers, \"\", text_body)\n",
    "    text_body = re.sub(clean_html_tags, \"\", text_body)\n",
    "    text_body = re.sub(clean_emails, \"\", text_body)\n",
    "    text_body = re.sub(clean_fwds, \" \", text_body)\n",
    "    text_body = re.sub(clean_unintended_sends, \" \", text_body)\n",
    "    text_body = re.sub(clean_dashes, \" \", text_body)\n",
    "    text_body = re.sub(clean_transmission_warn, \" \", text_body)\n",
    "    text_body = re.sub(clean_datetime, \" \", text_body)\n",
    "    text_body = re.sub(clean_link, \" \", text_body)\n",
    "    text_body = re.sub(clean_phone_fax, \" \", text_body)\n",
    "    text_body = re.sub(clean_phone_ctrycode, \" \", text_body)\n",
    "    text_body = re.sub(clean_addr_code, \" \", text_body)\n",
    "    text_body = re.sub(clean_email_codes, \"\", text_body)\n",
    "    text_body = re.sub(clean_very_long_text, \"\", text_body)\n",
    "    text_body = re.sub(clean_multi_symbols, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_space, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_symbols, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_space, \" \", text_body)\n",
    "\n",
    "    # this step saves the email body text as a value to the key named \"body\"\n",
    "    email_dict[\"body\"] = text_body.strip()\n",
    "\n",
    "    # append the dictionary to a list, and later store as a dataframe\n",
    "    list_of_dict.append(email_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99684f",
   "metadata": {},
   "source": [
    "We compile the dictionary into a dataframe and renamed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "370bfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df_feat = pd.DataFrame(list_of_dict)\n",
    "emails_df_feat.columns = ['Message-ID', 'DateTime', 'From', 'To', 'Subject', 'Cc', 'Mime-Version',  # Date -> DateTime\n",
    "                          'Content-Type', 'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To',\n",
    "                          'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName', 'body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8d9a9",
   "metadata": {},
   "source": [
    "We further clean up the email addresses in the \"From\". \"To\", \"Cc\", \"Bcc\" fields using Regex groups. E.g \"houston <.ward@enron.com>\", \"e-mail <.brandon@enron.com>\"; unlike the usual \"houston.ward@enron.com\".\n",
    "\n",
    "We then replace the columns with the cleaned up emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4528b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we do a clean up of email addresses\n",
    "reformat_emails = re.compile(r\"(?P<part1>[\\w-]+)[<\\s]*(?P<part2>[\\w.\\'\\W]+)(?P<domain>[@\\w.-]+)\")\n",
    "\n",
    "cleaned_from_emails = reformat_email_func(emails_df_feat, \"From\", reformat_emails)\n",
    "cleaned_to_emails = reformat_email_func(emails_df_feat, \"To\", reformat_emails)\n",
    "cleaned_cc_emails = reformat_email_func(emails_df_feat, \"Cc\", reformat_emails)\n",
    "cleaned_bcc_emails = reformat_email_func(emails_df_feat, \"Bcc\", reformat_emails)\n",
    "\n",
    "# - replace the columns with the cleaned up emails\n",
    "emails_df_feat.From = cleaned_from_emails\n",
    "emails_df_feat.To = cleaned_to_emails\n",
    "emails_df_feat.Cc = cleaned_cc_emails\n",
    "emails_df_feat.Bcc = cleaned_bcc_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14f96c",
   "metadata": {},
   "source": [
    "We create new columns to include reformatted data: date, time, domain name (From and To) for emails.\n",
    "\n",
    "It seems like \"strptime\" cannot handle Timezone codes directly and more steps are required. We are not going to need Timezone information for our purpose, so we are going to ignore them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199d0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df_feat[\"date\"] = emails_df_feat[\"DateTime\"].apply(lambda x: datetime\n",
    "                                                          .strptime(x[:-6], \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "                                                          .strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "emails_df_feat[\"time\"] = emails_df_feat[\"DateTime\"].apply(lambda x: datetime\n",
    "                                                          .strptime(x[:-6], \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "                                                          .strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "emails_df_feat[\"From_domain\"] = extract_domain(emails_df_feat, \"From\")\n",
    "emails_df_feat[\"To_domain\"] = extract_domain(emails_df_feat, \"To\")\n",
    "emails_df_feat[\"Cc_domain\"] = extract_domain(emails_df_feat, \"Cc\")\n",
    "emails_df_feat[\"Bcc_domain\"] = extract_domain(emails_df_feat, \"Bcc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb731df",
   "metadata": {},
   "source": [
    "### Quick glance at the resultant new dataframe (post-feature engineering)\n",
    "\n",
    "We take a quick look at some of the rows based on some sample name matches; we used \"apply\" together with \"str\" and \"contains\", and together with \"loc\", we can extract rows based on columns that have lists as elements https://www.chicagotribune.com/sns-ap-enron-trial-glance-story.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0370b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Message-ID  \\\n",
      "24  <21723779.1075857674036.JavaMail.evans@thyme>   \n",
      "40  <31890614.1075845545604.JavaMail.evans@thyme>   \n",
      "54  <13370742.1075851656808.JavaMail.evans@thyme>   \n",
      "71  <15840911.1075852366378.JavaMail.evans@thyme>   \n",
      "91  <25943015.1075858769769.JavaMail.evans@thyme>   \n",
      "\n",
      "                                 DateTime                       From  \\\n",
      "24   Fri, 8 Jun 2001 05:52:33 -0700 (PDT)          [black@enron.com]   \n",
      "40   Tue, 2 Jan 2001 07:38:00 -0800 (PST)  [john.lavorato@enron.com]   \n",
      "54  Fri, 12 Oct 2001 08:03:27 -0700 (PDT)    [ray.alvarez@enron.com]   \n",
      "71  Mon, 22 Oct 2001 08:22:03 -0700 (PDT)      [l..denton@enron.com]   \n",
      "91  Fri, 28 Sep 2001 19:05:56 -0700 (PDT)     [fran.chang@enron.com]   \n",
      "\n",
      "                                                   To  \\\n",
      "24  [tim.belden@enron.com, john.lavorato@enron.com...   \n",
      "40                         [david.delainey@enron.com]   \n",
      "54  [l..nicolay@enron.com, alan.comnes@enron.com, ...   \n",
      "71  [andrea.dahlke@enron.com, casey.evans@enron.co...   \n",
      "91  [tom.alonso@enron.com, tim.belden@enron.com, m...   \n",
      "\n",
      "                               Subject  \\\n",
      "24                        news stories   \n",
      "40                                       \n",
      "54  Operational Audit of CAISO by FERC   \n",
      "71          CNC Containers Corporation   \n",
      "91         09-28-01 West Power Reports   \n",
      "\n",
      "                                                   Cc Mime-Version  \\\n",
      "24                                                 []          1.0   \n",
      "40                                                 []          1.0   \n",
      "54                        [richard.shapiro@enron.com]          1.0   \n",
      "71                                                 []          1.0   \n",
      "91  [john.postlethwaite@enron.com, samantha.law@en...          1.0   \n",
      "\n",
      "                    Content-Type Content-Transfer-Encoding  \\\n",
      "24  text/plain; charset=us-ascii                      7bit   \n",
      "40  text/plain; charset=us-ascii                      7bit   \n",
      "54  text/plain; charset=us-ascii                      7bit   \n",
      "71  text/plain; charset=us-ascii                      7bit   \n",
      "91  text/plain; charset=us-ascii                      7bit   \n",
      "\n",
      "                                                  Bcc  \\\n",
      "24                                                 []   \n",
      "40                                                 []   \n",
      "54                        [richard.shapiro@enron.com]   \n",
      "71                                                 []   \n",
      "91  [john.postlethwaite@enron.com, samantha.law@en...   \n",
      "\n",
      "                                               X-From  \\\n",
      "24                                         Black, Don   \n",
      "40                                    John J Lavorato   \n",
      "54  Alvarez, Ray </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...   \n",
      "71  Denton, Rhonda L. </O=ENRON/OU=NA/CN=RECIPIENT...   \n",
      "91  Chang, Fran </O=ENRON/OU=NA/CN=RECIPIENTS/CN=F...   \n",
      "\n",
      "                                                 X-To  \\\n",
      "24  Belden, Tim </o=ENRON/ou=NA/cn=Recipients/cn=T...   \n",
      "40                                   David W Delainey   \n",
      "54  Nicolay, Christi L. </O=ENRON/OU=NA/CN=RECIPIE...   \n",
      "71  Dahlke, Andrea </O=ENRON/OU=NA/CN=RECIPIENTS/C...   \n",
      "91  Alonso, Tom </O=ENRON/OU=NA/CN=RECIPIENTS/CN=T...   \n",
      "\n",
      "                                                 X-cc X-bcc  \\\n",
      "24                                                            \n",
      "40                                                            \n",
      "54  Shapiro, Richard </O=ENRON/OU=NA/CN=RECIPIENTS...         \n",
      "71                                                            \n",
      "91  Postlethwaite, John </O=ENRON/OU=NA/CN=RECIPIE...         \n",
      "\n",
      "                                             X-Folder    X-Origin  \\\n",
      "24                             \\jlavora\\Deleted Items  Lavorado-J   \n",
      "40  \\John_Lavorato_Oct2001\\Notes Folders\\All docum...  LAVORATO-J   \n",
      "54  \\Dasovich, Jeff (Non-Privileged)\\Dasovich, Jef...  DASOVICH-J   \n",
      "71  \\JFORNEY (Non-Privileged)\\Forney, John M.\\Dele...    FORNEY-J   \n",
      "91             \\SWHITE (Non-Privileged)\\Deleted Items     White-S   \n",
      "\n",
      "                             X-FileName  \\\n",
      "24                          jlavora.pst   \n",
      "40                          jlavora.nsf   \n",
      "54  Dasovich, Jeff (Non-Privileged).pst   \n",
      "71         JFORNEY (Non-Privileged).pst   \n",
      "91          SWHITE (Non-Privileged).pst   \n",
      "\n",
      "                                                 body        date      time  \\\n",
      "24  Peggy Mahoney David W Delainey/HOU/, Janet R D...  2001-06-08  05:52:33   \n",
      "40    Do we need to talk about VP and MD nominations.  2001-01-02  07:38:00   \n",
      "54  Here are some bullets briefly describing the F...  2001-10-12  08:03:27   \n",
      "71  We received the executed EEI Master Power Purc...  2001-10-22  08:22:03   \n",
      "91  Canada's P&L will be updated on Monday (10/1) ...  2001-09-28  19:05:56   \n",
      "\n",
      "   From_domain To_domain Cc_domain Bcc_domain  \n",
      "24     [enron]   [enron]        []         []  \n",
      "40     [enron]   [enron]        []         []  \n",
      "54     [enron]   [enron]   [enron]    [enron]  \n",
      "71     [enron]   [enron]        []         []  \n",
      "91     [enron]   [enron]   [enron]    [enron]  \n"
     ]
    }
   ],
   "source": [
    "convicted_names = ['kenneth.lay','jeffrey.skilling','kevin.howard','michael.krautz','joe.hirko','rex.shelby',\n",
    "                   'scott.yeager','andrew.fastow','david.bermingham','giles.darby','gary.mulgrew','daniel.bayly',\n",
    "                   'james.brown','robert.furst','william.fuhs','dan.boyle','sheila.kahanek','christopher.calger',\n",
    "                   'richard.causey','lfastow','paula.rieker','ken.rice','mark.koenig','kevin.hannon','tim.despain',\n",
    "                   'jeff.richter','ben.glisan','david.delainey','michael.kopper','tim.belden','larry.lawyer',\n",
    "                   'david.duncan','wes.colwell','raymond.bowen']\n",
    "\n",
    "print(emails_df_feat.loc[(emails_df_feat.To.apply(lambda x : str(x)).str.contains('|'.join(convicted_names))), :].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17400301",
   "metadata": {},
   "source": [
    "### Find relationships using Sankey diagram\n",
    "\n",
    "We want to spot associations based on email address domains rather than names. The following part will prepare the data for the Sankey diagram plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0ddc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create the dataframe that will contain the \"source\" and \"destination\"\n",
    "source_domains = emails_df_feat.From_domain.to_list()\n",
    "dest_domains = [list(set(emails_df_feat.loc[num, \"To_domain\"])) for num in\n",
    "                range(emails_df_feat.shape[0])]  # de-duplicated\n",
    "\n",
    "# - introduce the source and destination lists and the dataframe with all the one-to-one mapping will be done\n",
    "sankey_source, sankey_destin, source_dest_map_dedup, counter = one_to_one_mapping(source_domains, dest_domains)\n",
    "\n",
    "# - obtain a dictionary to map the domain names into indices for purpose of plotting Sankey diagram\n",
    "full_list_of_domains = sorted(list(set(sankey_source + sankey_destin)))\n",
    "domain_dict = {domain: num for num, domain in enumerate(full_list_of_domains)}  # dictionary comprehension on key-value\n",
    "\n",
    "# - create the fields required for the Sankey diagram\n",
    "s2 = []  # to be directly used in plotting the Sankey diagram\n",
    "d2 = []  # to be directly used in plotting the Sankey diagram\n",
    "v2 = []  # to be directly used in plotting the Sankey diagram\n",
    "for i in range(len(source_dest_map_dedup)):\n",
    "    tag1 = source_dest_map_dedup[i]\n",
    "    s2.append(domain_dict[tag1[0]])\n",
    "    d2.append(domain_dict[tag1[1]])\n",
    "    v2.append(counter[tag1[0], tag1[1]])\n",
    "\n",
    "# - plot the sankey diagram\n",
    "random.seed(24)\n",
    "sample_vals = random.sample(range(0, len(s2)), 120)  # we sample 120 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cafee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = go.Figure(data=[go.Sankey(\n",
    "                    node = dict(\n",
    "                        pad = 5, thickness = 20, line = dict(color = \"black\", width = 0.5),\n",
    "                        label = full_list_of_domains, color = \"#3944BC\"\n",
    "                    ),\n",
    "                    link = dict(source = [s2[i] for i in sample_vals],\n",
    "                                target = [d2[i] for i in sample_vals],\n",
    "                                value = [v2[i] for i in sample_vals],\n",
    "                                color = \"#F699CF\"))])\n",
    "\n",
    "fig1.update_layout(title_text=\"Sankey Diagram to show associations based on email domains\", font_size=10)\n",
    "\n",
    "# fig1.update_layout(title_text=\"Sankey Diagram to show associations based on email domains\", font_size=10,\n",
    "#                   autosize=True, width=1200, height=1000)\n",
    "    \n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05bb943",
   "metadata": {},
   "source": [
    "###   Knowledge graph\n",
    "\n",
    " We now try to discover associations / information from email messages by building the Knowledge Graph.\n",
    "\n",
    "Building graphs requires nodes and edges; same goes for knowledge graphs. The nodes are going to be the entities mentioned in the sentences; edges are the relationships connecting the nodes. In this portion of the script, we also convert the email bodies into sentence tokens for us to extract the relations and the entity pairs.\n",
    "\n",
    "\n",
    "**NOTE: plotting Knowledge Graph for email messages may not be quite feasible given the way that emails are written, the reason for doing Knowledge Graph for this exercise is purely for learning purposes**\n",
    "\n",
    "**References:**\n",
    "- https://gist.github.com/quadrismegistus/92a7fba479fc1e7d2661909d19d4ae7e, \n",
    "- https://pyvis.readthedocs.io/en/latest/tutorial.html, https://github.com/WestHealth/pyvis,\n",
    "- https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7836ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create the required \"document\"; we further create sentence tokens to extract the entities and relationships\n",
    "email_doc_sentences = sent_tokenize(' '.join(emails_df_feat.body.to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff77cb",
   "metadata": {},
   "source": [
    "We extract the entities and relations (if the entity pair does not contain just a 'blank' entity). This for loop takes a while, we use TQDM here to track its progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6caa70d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35059/35059 [28:17<00:00, 20.65it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_pairs = []\n",
    "relations = []\n",
    "exclusion_list = ['','you','i','me','them','we','they','it','this','who','us','he','that','she','what','>']\n",
    "for i in tqdm(email_doc_sentences):\n",
    "    if (get_entities(i)[0].lower() not in exclusion_list) and (get_entities(i)[1].lower() not in exclusion_list):\n",
    "        entity_pairs.append(get_entities(i))\n",
    "        relations.append(get_relation(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed653da",
   "metadata": {},
   "source": [
    "We print the top 50 entity pairs and top 50 relations; choose one to plot for the Knowledge Graph later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aac29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(entity_pairs).value_counts().head(50))\n",
    "print(pd.Series(relations).value_counts().head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb74ce",
   "metadata": {},
   "source": [
    "We now create the **dataframe for Knowledge Graph**. In the dataframe, we also include some colors and images for better visualization of the names of interest amongst the sea of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create the dataframe for Knowledge Graph\n",
    "source_kg = [s[0] for s in entity_pairs]\n",
    "destin_kg = [d[1] for d in entity_pairs]\n",
    "know_df = pd.DataFrame({'source': source_kg, 'destination': destin_kg, 'edge': relations})\n",
    "\n",
    "# - identify if the nodes contain names of interest (based on Wikipedia, the C-suite officers)\n",
    "name_patterns = '[Ee]nron|[Bb]yron|[Kk]enneth|[Jj]effrey|[Ss]killing|[Aa]ndrew|[Ff]astow'\n",
    "\n",
    "# - we create the filtered dataframe for Knowledge Graph based on name matching patterns\n",
    "filtered_know_df = know_df.loc[(know_df.source.str.contains(name_patterns)) |\n",
    "                               know_df.destination.str.contains(name_patterns), :].reset_index(drop=True)\n",
    "\n",
    "sample_idx1 = random.sample(range(0, filtered_know_df.shape[0]), 120)  # we sample 120 entries\n",
    "filtered_know_df = filtered_know_df.loc[sample_idx1, :].reset_index(drop=True)\n",
    "\n",
    "# - create the nodes list with the colours; nodes are coloured if they contain the names of interest\n",
    "full_nodes = filtered_know_df.source.to_list() + filtered_know_df.destination.to_list()\n",
    "color_nodes = ['red' if re.findall(name_patterns, i.lower()) != [] else '#3944BC' for i in full_nodes]\n",
    "image_nodes = [\"/home/kelvinhwee/PycharmProjects/enronFraudEmailAnalysis/bad guy.JPG\"\n",
    "               if re.findall(name_patterns, i.lower()) != []\n",
    "               else \"/home/kelvinhwee/PycharmProjects/enronFraudEmailAnalysis/good guy.JPG\" for i in full_nodes]\n",
    "\n",
    "nodes_color_df = pd.DataFrame({'node': full_nodes, 'color': color_nodes, 'image': image_nodes})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff153f",
   "metadata": {},
   "source": [
    "Let's now **plot the knowledge graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fe184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plot the Knowledge Graph: initialise the networkx graph object\n",
    "G_know = nx.Graph()\n",
    "\n",
    "# - plot the Knowledge Graph: add nodes (do we want to consider the weightage)\n",
    "for i in range(nodes_color_df.shape[0]):\n",
    "    G_know.add_node(nodes_color_df[\"node\"][i], color=nodes_color_df[\"color\"][i],\n",
    "                    shape='image',\n",
    "                    image=nodes_color_df[\"image\"][i])\n",
    "\n",
    "# - plot the Knowledge Graph: add edges (label the edges with the relation)\n",
    "for i in range(filtered_know_df.shape[0]):\n",
    "    G_know.add_edge(filtered_know_df[\"source\"][i], filtered_know_df[\"destination\"][i],\n",
    "                    label=filtered_know_df[\"edge\"][i], title=filtered_know_df[\"edge\"][i])\n",
    "\n",
    "# - plot the final graph using Pyvis (https://pyvis.readthedocs.io/en/latest/documentation.html)\n",
    "nt_know = Network(height=1000, width=1200, directed=True)\n",
    "nt_know.toggle_hide_edges_on_drag(True)\n",
    "\n",
    "# - BarnesHut is a quadtree based gravity model. It is the fastest\n",
    "nt_know.barnes_hut(spring_length=10, overlap=0.5, gravity=-10000, central_gravity=0.8)\n",
    "nt_know.from_nx(G_know)\n",
    "nt_know.show(\"knowledge_graph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3f92b",
   "metadata": {},
   "source": [
    "### Network graph \n",
    "\n",
    "We want to show the connections between various parties using network visualisations. Later on we will do some centrality scoring of parties and plot graph based on top three parties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61517db",
   "metadata": {},
   "source": [
    "We now create the **dataframe for Network Graph**. In the dataframe, we also include some colors and images for better visualization of the names of interest amongst the sea of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65747a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - get the one-to-one mapping for the email addresses\n",
    "email_sender_list = emails_df_feat.From.to_list()\n",
    "email_recipient_list = emails_df_feat.To.to_list()\n",
    "net_source, net_destin, source_dest_net_map, _  = one_to_one_mapping(email_sender_list, email_recipient_list)\n",
    "\n",
    "# - create the dataframe for Network Graph\n",
    "net_df = pd.DataFrame({'source': net_source, 'destination': net_destin})\n",
    "\n",
    "# - we create the filtered dataframe for Network Graph and drop 'blank' destinations and the self-sending ones\n",
    "filtered_net_df = net_df.loc[net_df.destination != '', :].reset_index(drop = True)\n",
    "filtered_net_df = filtered_net_df.loc[filtered_net_df.source != filtered_net_df.destination, :].reset_index(drop = True)\n",
    "\n",
    "# - we identify which rows contain some of the names of interest (based on Wikipedia, the C-suite officers)\n",
    "name_patterns_net = '[Ss]killing|[Ff]astow|[Jj]usbasche|[Cc]ooper|[Bb]elden'\n",
    "interest_idx = filtered_net_df.loc[filtered_net_df.source.str.contains(name_patterns_net) |\n",
    "                                   filtered_net_df.destination.str.contains(name_patterns_net), :].head(300).index\n",
    "\n",
    "# - we identify names that we want to exclude\n",
    "name_patterns_excl = '[Tt]echnology|outlook.team'\n",
    "excl_idx = filtered_net_df.loc[filtered_net_df.source.str.contains(name_patterns_excl) |\n",
    "                               filtered_net_df.destination.str.contains(name_patterns_excl), :].index\n",
    "\n",
    "# - complete the list\n",
    "list_of_idx = [num for num in range(filtered_net_df.shape[0]) if num not in list(excl_idx) + list(interest_idx)]\n",
    "\n",
    "# - we take a sample of the dataframe for plotting the graph\n",
    "sample_idx2 = random.sample(list_of_idx, min(3000, len(list_of_idx))) + list(interest_idx) # sample the emails\n",
    "\n",
    "# - final sample dataframe for plotting\n",
    "sample_net_df = filtered_net_df.loc[sample_idx2 + list(interest_idx), :].reset_index(drop=True)\n",
    "\n",
    "# - create the list of source and destination nodes\n",
    "full_nodes_net = sample_net_df.source.to_list() + sample_net_df.destination.to_list()\n",
    "\n",
    "# - we colour based on emails that contain names of interest, and whether the email address is an 'Enron' email\n",
    "color_nodes_net = []\n",
    "for i in full_nodes_net:\n",
    "    if re.findall(name_patterns_net, i.lower()):\n",
    "        color_nodes_net.append('red')\n",
    "    elif 'enron' in i.lower():\n",
    "        color_nodes_net.append('#AEF359')\n",
    "    else:\n",
    "        color_nodes_net.append('#3944BC')\n",
    "\n",
    "nodes_color_net_df = pd.DataFrame({'node': full_nodes_net, 'color': color_nodes_net})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736784c",
   "metadata": {},
   "source": [
    "Let us now **plot the network graph.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d88081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plot the Network Graph: initialise the networkx graph object\n",
    "G_net = nx.Graph()\n",
    "\n",
    "# - plot the Network Graph: add nodes (do we want to consider the weightage)\n",
    "for i in range(nodes_color_net_df.shape[0]):\n",
    "    G_net.add_node(nodes_color_net_df[\"node\"][i], color=nodes_color_net_df[\"color\"][i], layout = 'hierarchical')\n",
    "\n",
    "# - plot the Network Graph: add edges (label the edges with the relation)\n",
    "for i in range(sample_net_df.shape[0]):\n",
    "    G_net.add_edge(sample_net_df[\"source\"][i], sample_net_df[\"destination\"][i])\n",
    "\n",
    "# - plot the final graph using Pyvis (https://pyvis.readthedocs.io/en/latest/documentation.html)\n",
    "nt_network = Network(height=1000, width=1200, directed=True)\n",
    "nt_network.toggle_hide_edges_on_drag(False)\n",
    "nt_network.from_nx(G_net)\n",
    "nt_network.show(\"network_graph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34097557",
   "metadata": {},
   "source": [
    "### Centrality scoring of the nodes\n",
    "\n",
    "We attempt to do the scoring based on three common centrality scoring methods:\n",
    "- degree centrality\n",
    "- closeness centrality\n",
    "- betweenness centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e4989",
   "metadata": {},
   "source": [
    "**Degree centrality** is the most basic way of computing centrality. It simply measures the total number of connections linked to a node within a network graph. The degree centrality values are normalized by dividing by the maximum possible degree in a simple graph n-1 where n is the number of nodes in G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b999b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we try to derive the degree centrality score of the nodes\n",
    "deg_cent_G1 = nx.degree_centrality(G_net)\n",
    "deg_g1_df = pd.DataFrame(deg_cent_G1.items(), columns=['Emails', 'Deg_centrality'])\\\n",
    "                            .sort_values(by = \"Deg_centrality\", ascending=False)\n",
    "\n",
    "print('We look at the top 10 email addresses (for degree centrality): ', deg_g1_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811826ed",
   "metadata": {},
   "source": [
    "**Closeness centrality** is a measure of centrality in a network, calculated as the reciprocal of the sum of the length of the shortest paths between the node and all other nodes in the graph. Thus, the more central a node is, the closer it is to all other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39029167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we try to derive the closeness centrality score of the nodes\n",
    "close_cent_G1 = nx.closeness_centrality(G_net)\n",
    "close_g1_df = pd.DataFrame(close_cent_G1.items(), columns=['Emails', 'Closeness_centrality'])\\\n",
    "                            .sort_values(by = \"Closeness_centrality\", ascending=False)\n",
    "\n",
    "print('We look at the top 10 email addresses (for closeness centrality): ', close_g1_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a310f7",
   "metadata": {},
   "source": [
    "**Betweenness centrality** is a way of detecting the amount of influence a node has over the flow of information in a graph. It is often used to find nodes that serve as a bridge from one part of a graph to another. The algorithm calculates unweighted shortest paths between all pairs of nodes in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a339ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we try to derive the between centrality score of the nodes\n",
    "btwn_cent_G1 = nx.betweenness_centrality(G_net)\n",
    "btwn_g1_df = pd.DataFrame(btwn_cent_G1.items(), columns=['Emails', 'Betweeness_centrality'])\\\n",
    "                            .sort_values(by = \"Betweeness_centrality\", ascending=False)\n",
    "\n",
    "print('We look at the top 10 email addresses (for betweeness centrality): ', btwn_g1_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f1dd0",
   "metadata": {},
   "source": [
    "Having done the above, we want to take a look based on all three measures of centrality, which are the top few email addresses that appear to be \"most influential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be79679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we combine the scores and return the top 3 and its graph\n",
    "cent_scores_df = deg_g1_df.merge(close_g1_df, how='left', on='Emails').merge(btwn_g1_df, how='left', on='Emails')\n",
    "cent_scores_df[\"Total_scores\"] = cent_scores_df.Deg_centrality * \\\n",
    "                                 cent_scores_df.Closeness_centrality * \\\n",
    "                                 cent_scores_df.Betweeness_centrality\n",
    "\n",
    "cent_scores_df = cent_scores_df.sort_values(by=\"Total_scores\", ascending=False)\n",
    "\n",
    "print('We look at the top 10 email addresses (for total score): ', cent_scores_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55483254",
   "metadata": {},
   "source": [
    "We now create the dataframe for Network Graph for the **most influential nodes**. In the dataframe, we also include some colors and images for better visualization of the names of interest amongst the sea of nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90363bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plot graph for the top 3 most central nodes\n",
    "top3_emails = cent_scores_df.loc[:2, \"Emails\"].to_list()\n",
    "\n",
    "top3_net_df = filtered_net_df.loc[filtered_net_df.source.isin(top3_emails) |\n",
    "                                  filtered_net_df.destination.isin(top3_emails), :].reset_index(drop=True)\n",
    "\n",
    "# - create the list of source and destination nodes\n",
    "top3_nodes_net = top3_net_df.source.to_list() + top3_net_df.destination.to_list()\n",
    "\n",
    "# - we colour based on emails that contain names of interest, and whether the email address is an 'Enron' email\n",
    "top3_color_nodes_net = []\n",
    "for i in top3_nodes_net:\n",
    "    if i in top3_emails:\n",
    "        top3_color_nodes_net.append('red')\n",
    "    elif 'enron' in i.lower():\n",
    "        top3_color_nodes_net.append('#AEF359')\n",
    "    else:\n",
    "        top3_color_nodes_net.append('#3944BC')\n",
    "\n",
    "top3_nodes_color_net_df = pd.DataFrame({'node': top3_nodes_net, 'color': top3_color_nodes_net})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180ce81",
   "metadata": {},
   "source": [
    "Let us now plot the network graph for the **most influential nodes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - plot the Network Graph: initialise the networkx graph object\n",
    "G_net_top3 = nx.Graph()\n",
    "\n",
    "# - plot the Network Graph: add nodes (do we want to consider the weightage)\n",
    "for i in range(top3_nodes_color_net_df.shape[0]):\n",
    "    G_net_top3.add_node(top3_nodes_color_net_df[\"node\"][i],\n",
    "                        color=top3_nodes_color_net_df[\"color\"][i],\n",
    "                        layout = 'hierarchical')\n",
    "\n",
    "# - plot the Network Graph: add edges (label the edges with the relation)\n",
    "for i in range(top3_net_df.shape[0]):\n",
    "    G_net_top3.add_edge(top3_net_df[\"source\"][i], top3_net_df[\"destination\"][i])\n",
    "\n",
    "# - plot the final graph using Pyvis (https://pyvis.readthedocs.io/en/latest/documentation.html)\n",
    "top3_network = Network(height=1000, width=1200, directed=True)\n",
    "top3_network.toggle_hide_edges_on_drag(False)\n",
    "top3_network.from_nx(G_net_top3)\n",
    "top3_network.show(\"network_graph_top3.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec83543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91197854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
