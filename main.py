#######################################################################################################################
#   Background of code
#######################################################################################################################
'''
We attempt to use multiple linear regression models to model the insurance premium prediction problem
'''

#######################################################################################################################
#   Basic configuration steps
#######################################################################################################################

# - import basic python packages
import warnings
import tkinter  # to show plot in Pycharm

warnings.simplefilter(action='ignore', category=FutureWarning)

# - import packages for data manipulations
import numpy as np
import pandas as pd
from datetime import date, datetime
from utils import extract_domain

# - import packages for visualisation
from pyvis.network import Network
import networkx as nx
import matplotlib.pyplot as plt

# - import packages for NLP
'''
- for the installation of "en_core_web_sm", we use the following command in terminal
- pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0.tar.gz
- the file was then downloaded to this folder: 
- /home/kelvinhwee/.cache/pip/wheels/62/79/40/648305f0a2cd1fdab236bd6764ba467437c5fae2a925768153 
- (look out for the installation completion message in the terminal)
- we copied the zipped file, and extracted the "en_core_web_sm-3.1.0" folder (containing the "config.cfg" file) into the 
'''
import spacy
import re

# - other configurations
pd.set_option("display.max_column", None)
source_filepath = '/home/kelvinhwee/PycharmProjects/sourceFiles'

# - we load the spacy trained pipelines (for English); this is an English pipeline optimized for CPU
nlp = spacy.load('en_core_web_sm-3.1.0')

# - packages created


########################################################################################################################
#   Read CSV data file
########################################################################################################################
'''
- The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. 
- It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.
- This is the May 7, 2015 Version of dataset, as published at https://www.cs.cmu.edu/~./enron/
- we note that there are only two columes, "file" and "message"
'''

# # - read in the CSV data
# emails_df = pd.read_csv(source_filepath + '/emails.csv')
#
# # - read in the sample CSV data
# import random
# sample_vals = random.sample(list(range(emails_df.shape[0])), 1000)
# emails_df.loc[sample_vals].to_csv(source_filepath + '/sample_emails.csv')

emails_df = pd.read_csv(source_filepath + '/sample_emails.csv')
print("We look at a sample of the data: \n", emails_df.head(10))


# - we take a look at some specific instances of "message"
print(emails_df["message"][8])  # "X-From" and "X-To" field corresponds to the "From" and "To", but more explicit
print(emails_df["message"][88])  # "Subject" field seems to be blank
# print(emails_df["message"][888])  # email with several recipients (sent to an email group)


########################################################################################################################
#   Feature engineering - extract critical data points from email messages
########################################################################################################################

# - replace some characters
for i in range(emails_df.shape[0]):
    temp_text = emails_df["message"][i]
    new_text = temp_text.replace("\n ", " ")  # some "\n " in subject; , clean them to space character
    # new_text = new_text.replace("\n\n", "\n")  # dropped this; the one after "filename" always has double "\n"
    new_text = new_text.replace("Re: ", "")  # some "Re: " in subject; , clean them to blanks
    new_text = new_text.replace("Fw: ", "")  # some "Fw: " in subject; , clean them to blanks
    new_text = new_text.replace("\n\t", "") # very long recipient list has "\n\t"; clean them to blanks
    new_text = new_text.replace(" : ", "") # some ":" in subject; , clean them to blanks
    new_text = new_text.replace("[IMAGE]", "") # some "[IMAGE]" tags; , clean them to blanks
    emails_df.loc[i, "message"] = new_text

# - we collate the list of "keys"
keys_list = ['Message-ID', 'Date', 'From', 'To', 'Subject', 'Cc', 'Mime-Version', 'Content-Type',
             'Content-Transfer-Encoding','Bcc','X-From','X-To','X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName']

fields_list = keys_list + ["Sent"] # to add in additional fields to clean (for RegEx later)
fields_list_plus = fields_list + [i.lower() for i in fields_list] # include variations of 'lower case'

# - create dictionary (using dictionary comprehension) to do "conversion" later on (you will see)
keys_dict = {i:[k, len(k)] for i, k in enumerate(keys_list)}

# - we try to do a batch-wise extraction of the email contents based on the placeholders e.g. "To", "From", "Subject"
list_of_dict = []
for i in range(emails_df.shape[0]): # i=4

    email_dict = {} # empty dictionary to store the key-value pair
    temp_str = emails_df["message"][i].split("\n") # assign string to variable; so can insert values to specific place

    # this step uses the above created dictionary to "impute" keys if there are missing key values, e.g. "To", "Cc"
    for pos in range(len(keys_list)):
        if temp_str[pos][0:len(keys_dict[pos][0])] != keys_dict[pos][0]:
            temp_str.insert(pos, str(keys_dict[pos][0]) + ': ')

    # this step performs the split and extract the key-value pair for the standard known field headers
    for j in range(0, 17):
        key = temp_str[j].split(":")[0]
        val = ':'.join(temp_str[j].split(":")[1:]).strip()
        email_dict[key] = val

    # this step saves the body of the text; we apply some regex logic
    text_body = temp_str[17: ]
    text_body = " ".join([text for text in text_body]).strip() # joins back all elements into a single string

    # regex logic
    clean_html_tags = re.compile("<[/]*.*?>|&nbsp;")
    clean_multi_space = re.compile("[\s]{2,}")
    clean_field_headers = re.compile('|'.join([item + ":" for item in fields_list_plus]))
    clean_emails = re.compile("[\w._]+@[\w.]+")
    clean_fwds = re.compile("[-]{2,}.*?[-]{2,}") # cleans the "Forwarded by" in between the long dashes
    clean_dashes = re.compile("[-]{2,}")
    clean_transmission_warn = re.compile(r"The information.*?any computer.") # cleans warning texts
    clean_datetime = re.compile("[\d]{1,2}/[\d]{1,2}/[\d]{4}\s+[\d]{1,2}:[\d]{1,2}[:\d]*\s+[AMPM]+") # for datetime format DD/MM/YYYY XX:XX:XX AM/PM
    clean_multi_symbols = re.compile("[>,(]+\s+[>,(]+") # e.g. "> >", ", , ", ", ("
    clean_addr_code = re.compile("[, ]*[A-Z]{2}\s+[\d]{5}")  # cleans ", TX 77082"
    clean_phone_fax = re.compile("[\d]*[-]*[\d]{3}-[\d]{3}-[\d]{4}[\s]*[(]*\w*[)]*") # "713-853-3989 (Phone)", "713-646-3393(Fax", "1-888-334-4204"
    clean_phone_ctrycode = re.compile("\([\d]{3}\)[\s]*[\d]{3}-[\d]{4}") # (281) 558-9198
    clean_link = re.compile(r"http[s]*://+[\w]+[.][\w]+[.][\w]+") # e.g. http://explorer.msn.com
    # Other things to clean: Staff Meeting - Mt. Ranier 5/30/2001 Time: 1:00 PM - 3:00 PM (Central Standard Time)

    # apply regex logic
    text_body = re.sub(clean_field_headers, "", text_body)
    text_body = re.sub(clean_html_tags, "", text_body)
    text_body = re.sub(clean_emails, "", text_body)
    text_body = re.sub(clean_fwds, " ", text_body)
    text_body = re.sub(clean_dashes, " ", text_body)
    text_body = re.sub(clean_transmission_warn, " ", text_body)
    text_body = re.sub(clean_datetime, " ", text_body)
    text_body = re.sub(clean_link, " ", text_body)
    text_body = re.sub(clean_phone_fax, " ", text_body)
    text_body = re.sub(clean_addr_code, " ", text_body)
    text_body = re.sub(clean_multi_symbols, " ", text_body)
    text_body = re.sub(clean_multi_space, " ", text_body)

    # this step saves the email body text as a value to the key named "body"
    email_dict["body"] = text_body

    # append the dictionary to a list, and later store as a dataframe
    list_of_dict.append(email_dict)

# - we compile the dictionary into a dataframe (rename columns) and print a few examples to take a look
emails_df_feat = pd.DataFrame(list_of_dict)
emails_df_feat.columns = ['Message-ID', 'DateTime', 'From', 'To', 'Subject', 'Cc', 'Mime-Version', # Date -> DateTime
                          'Content-Type', 'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To',
                          'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName', 'body']

print(emails_df_feat.head())


# - create new columns to include reformatted data: date, time, domain name (From and To) for emails
'''
- it seems like strptime cannot handle Timezone codes directly and more steps are required
- we are not going to need Timezone information for our purpose, so we are going to ignore them
'''
emails_df_feat["date"] = emails_df_feat["DateTime"].apply(lambda x: datetime
                                                          .strptime(x[:-6], "%a, %d %b %Y %H:%M:%S %z")
                                                          .strftime("%Y-%m-%d"))

emails_df_feat["time"] = emails_df_feat["DateTime"].apply(lambda x: datetime
                                                          .strptime(x[:-6], "%a, %d %b %Y %H:%M:%S %z")
                                                          .strftime("%H:%M:%S"))

emails_df_feat["From_domain"] = extract_domain(emails_df_feat, "From")
emails_df_feat["To_domain"] = extract_domain(emails_df_feat, "To")
emails_df_feat["Cc_domain"] = extract_domain(emails_df_feat, "Cc")
emails_df_feat["Bcc_domain"] = extract_domain(emails_df_feat, "Bcc")


########################################################################################################################
#   Find anomalous relationships - spot associations based on email addresses
########################################################################################################################

#=== we use different ways to plot the associations (Sankey diagram, network graphs)

#--- we try to plot the Sankey diagram
'''
- we determine the "source" and "destinations" and we do this using the "email domain"
- the "source" will comprise the "From" domains and the "destination" will comprise the others, i.e. "To", "Cc", "Bcc"
'''

source_domains = emails_df_feat.From_domain
dest_domains = [list(set(emails_df_feat.loc[num, "To_domain"] +
                emails_df_feat.loc[num, "Cc_domain"] +
                emails_df_feat.loc[num, "Bcc_domain"]))
                for num in range(emails_df_feat.shape[0])]




'''
if an email contains forwarded messages, it seems like the original messages are not found in other parts of the data
"emails_df_feat.Subject.value_counts().head(20)" does not show much repeats
'''

doc = nlp(emails_df["message"][888])
for tok in doc:
    print(tok.text, "...", tok.dep_)

'''
scattertext === https://github.com/JasonKessler/scattertext
'''
