{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503e0bbd",
   "metadata": {},
   "source": [
    "###   Background of code\n",
    "\n",
    "\n",
    "We attempt to discover or mine information from the email dataset in different ways / visualization methods.\n",
    "\n",
    "**For this Jupyter Notebook, all graphics are interactive. But because of the challenge in rendering interactive graphics in Github, the visualisations are not shown. It is recommended that you download and run the Jupyter Notebook at your own time :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0f1d2",
   "metadata": {},
   "source": [
    "###   Basic configuration steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065cc775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kelvinhwee/PycharmProjects/enronFraudEmailAnalysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e3e19",
   "metadata": {},
   "source": [
    "### We install some packages\n",
    "Packages for basic processing, data manipulation, visualisation and graphing of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1abeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - import basic python packages\n",
    "import warnings\n",
    "import tkinter  # to show plot in Pycharm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# - import packages for data manipulations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# - import packages for visualisation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"browser\"\n",
    "# from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5bf76d",
   "metadata": {},
   "source": [
    "### Additionally, we install packages for NLP\n",
    "\n",
    "For the installation of \"en_core_web_sm\", we use the following command in terminal \"pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0.tar.gz\". The file was then downloaded to this folder: \"/home/kelvinhwee/.cache/pip/wheels/62/79/40/648305f0a2cd1fdab236bd6764ba467437c5fae2a925768153\"\n",
    "(look out for the installation completion message in the terminal).\n",
    "\n",
    "Lastly, we copied the zipped file, and extracted the \"en_core_web_sm-3.1.0\" folder (containing the \"config.cfg\" file) into the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d06b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kelvinhwee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # uncomment this if you run into punkt download issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7a3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we load the spacy trained pipelines (for English); this is an English pipeline optimized for CPU\n",
    "nlp = spacy.load('en_core_web_sm-3.1.0')\n",
    "\n",
    "# - initialise the spacy Matcher with a vocab; matcher must always share the same vocab with the documents it operate on\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b8b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - other configurations\n",
    "pd.set_option(\"display.max_column\", None)\n",
    "source_filepath = '/home/kelvinhwee/PycharmProjects/sourceFiles'\n",
    "\n",
    "# - packages created\n",
    "from utils import extract_domain, reformat_email_func\n",
    "from utils import one_to_one_mapping\n",
    "from utils import get_relation, get_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8132d",
   "metadata": {},
   "source": [
    "### Read CSV data file\n",
    "\n",
    "The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.\n",
    "\n",
    "This is the May 7, 2015 Version of dataset, as published at https://www.cs.cmu.edu/~./enron/. we note that there are only two columes, \"file\" and \"message\".\n",
    "\n",
    "For this exercise, we have used a smaller sample set which we derived from the full data (commands to load full data have been commented out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8e349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We look at a sample of the data: \n",
      "    Unnamed: 0                              file  \\\n",
      "0      237243          kean-s/attachments/1757.   \n",
      "1       63803    dasovich-j/all_documents/4101.   \n",
      "2      320451                 mann-k/sent/3197.   \n",
      "3      367550             quigley-d/fin_desk/6.   \n",
      "4       83252              dasovich-j/sent/700.   \n",
      "5      476834      taylor-m/all_documents/7964.   \n",
      "6      465472       symes-k/all_documents/3742.   \n",
      "7      112944       fischer-m/deleted_items/17.   \n",
      "8      107663  farmer-d/discussion_threads/553.   \n",
      "9      114674           forney-j/sent_items/95.   \n",
      "\n",
      "                                             message  \n",
      "0  Message-ID: <27407479.1075851045148.JavaMail.e...  \n",
      "1  Message-ID: <301250.1075843054571.JavaMail.eva...  \n",
      "2  Message-ID: <15348104.1075845996347.JavaMail.e...  \n",
      "3  Message-ID: <9943936.1075841449972.JavaMail.ev...  \n",
      "4  Message-ID: <32384912.1075843200467.JavaMail.e...  \n",
      "5  Message-ID: <23924850.1075860197196.JavaMail.e...  \n",
      "6  Message-ID: <13474213.1075841707824.JavaMail.e...  \n",
      "7  Message-ID: <20240491.1075853098146.JavaMail.e...  \n",
      "8  Message-ID: <31574274.1075854056946.JavaMail.e...  \n",
      "9  Message-ID: <12793475.1075852369729.JavaMail.e...  \n"
     ]
    }
   ],
   "source": [
    "# # - read in the CSV data\n",
    "# emails_df = pd.read_csv(source_filepath + '/emails.csv')\n",
    "#\n",
    "# - read in the sample CSV data\n",
    "# import random\n",
    "# sample_vals = random.sample(list(range(emails_df.shape[0])), 5000)\n",
    "# emails_df.loc[sample_vals].to_csv(source_filepath + '/sample_emails.csv')\n",
    "\n",
    "emails_df = pd.read_csv(source_filepath + '/sample_emails.csv')\n",
    "print(\"We look at a sample of the data: \\n\", emails_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2ac38",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Extract critical data points from email messages. For a start, we try to replace some characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca987957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(emails_df.shape[0]):\n",
    "    temp_text = emails_df[\"message\"][i]\n",
    "    new_text = temp_text.replace(\"\\n \", \" \")  # some \"\\n \" in subject; , clean them to space character\n",
    "    # new_text = new_text.replace(\"\\n\\n\", \"\\n\")  # dropped this; the one after \"filename\" always has double \"\\n\"\n",
    "    new_text = new_text.replace(\"Re: \", \"\")  # some \"Re: \" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"Fw: \", \"\")  # some \"Fw: \" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"\\n\\t\", \"\")  # very long recipient list has \"\\n\\t\"; clean them to blanks\n",
    "    new_text = new_text.replace(\" : \", \"\")  # some \":\" in subject; , clean them to blanks\n",
    "    new_text = new_text.replace(\"[IMAGE]\", \"\")  # some \"[IMAGE]\" tags; , clean them to blanks\n",
    "    emails_df.loc[i, \"message\"] = new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b85a89",
   "metadata": {},
   "source": [
    "Next we try to collate the list of fields present in the email message and then we create a dictionary object to do some information extraction for processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65b733c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we collate the list of \"keys\"\n",
    "keys_list = ['Message-ID', 'Date', 'From', 'To', 'Subject', 'Cc', 'Mime-Version', 'Content-Type',\n",
    "             'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin',\n",
    "             'X-FileName']\n",
    "\n",
    "fields_list = keys_list + [\"Sent\"]  # to add in additional fields to clean (for RegEx later)\n",
    "fields_list_plus = fields_list + [i.lower() for i in fields_list] \\\n",
    "                   + [i.upper() for i in fields_list]  # include variations of lower and upper case\n",
    "\n",
    "# - create dictionary (using dictionary comprehension) to do \"conversion\" later on (you will see)\n",
    "keys_dict = {i: [k, len(k)] for i, k in enumerate(keys_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf853e6",
   "metadata": {},
   "source": [
    "We try to perform a batch-wise extraction of the email contents based on the placeholders e.g. \"To\", \"From\", \"Subject\". We collate the list of regex logic first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f48e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex logic\n",
    "clean_html_tags = re.compile(\"<[/]*.*?>|&nbsp;\")\n",
    "clean_multi_space = re.compile(\"[\\s]{2,}\")\n",
    "clean_field_headers = re.compile('|'.join([item + \":\" for item in fields_list_plus]))\n",
    "clean_emails = re.compile(\"[\\w._]+@[\\w.]+\")\n",
    "clean_fwds = re.compile(\"[-_]{2,}.*?[-_]{2,}|FW:|Fwd:|RE:\")  # cleans \"Forwarded by\" in between long dashes and others\n",
    "clean_unintended_sends = re.compile(\"[-_*]{2,}.*?[-_*]{2,}\")\n",
    "clean_dashes = re.compile(\"[-]{2,}\")\n",
    "clean_transmission_warn = re.compile(r\"The information.*?any computer.\")  # cleans warning texts\n",
    "clean_datetime = re.compile(\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\\s+[\\d]{1,2}:[\\d]{1,2}[:\\d]*\\s+[AMPM]+\")  # for format DD/MM/YYYY XX:XX:XX AM/PM\n",
    "clean_multi_symbols = re.compile(\"[>,(\\\"\\'\\\\!.\\[\\]-]+\\s?[>,(\\\"\\'\\\\!.\\[\\]-]+\")  # e.g. \"> >\", \", , \", \", (\"\n",
    "clean_addr_code = re.compile(\"[, ]*[A-Z]{2}\\s+[\\d]{5}\")  # cleans \", TX 77082\"\n",
    "clean_phone_fax = re.compile(\"[\\d]*[-]*[\\d]{3}-[\\d]{3}-[\\d]{4}[\\s]*[(]*\\w*[)]*\")  # \"713-853-3989 (Phone)\", \"713-646-3393(Fax\", \"1-888-334-4204\"\n",
    "clean_phone_ctrycode = re.compile(\"\\([\\d]{3}\\)[\\s]*[\\d]{3}-[\\d]{4}\")  # (281) 558-9198, (713) 670-2457\n",
    "clean_link = re.compile(r\"[http]*[https]*[:/]*/?[\\w]+[.][\\w]+.*[.][\\w]+\")  # e.g. http://explorer.msn.com, https://explorer.msn.com.net\"\n",
    "clean_email_codes = re.compile(\"[=][\\d]+\")  # clear email codes \"=19\", \"=20\"\n",
    "clean_very_long_text = re.compile(\"[\\w+]{20,}\")\n",
    "# Other things to clean: Staff Meeting - Mt. Ranier 5/30/2001 Time: 1:00 PM - 3:00 PM (Central Standard Time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ebd6c",
   "metadata": {},
   "source": [
    "We now perform the batch-wise extraction of email contents using the regex logic compiled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac3f2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dict = []\n",
    "for i in range(emails_df.shape[0]):  # i=4\n",
    "\n",
    "    email_dict = {}  # empty dictionary to store the key-value pair\n",
    "    temp_str = emails_df[\"message\"][i].split(\"\\n\")  # assign string to variable; so can insert values to specific place\n",
    "\n",
    "    # this step uses the above created dictionary to \"impute\" keys if there are missing key values, e.g. \"To\", \"Cc\"\n",
    "    for pos in range(len(keys_list)):\n",
    "        if temp_str[pos][0:len(keys_dict[pos][0])] != keys_dict[pos][0]:\n",
    "            temp_str.insert(pos, str(keys_dict[pos][0]) + ': ')\n",
    "\n",
    "    # this step performs the split and extract the key-value pair for the standard known field headers\n",
    "    for j in range(0, 17):\n",
    "        key = temp_str[j].split(\":\")[0]\n",
    "        val = ':'.join(temp_str[j].split(\":\")[1:]).strip()\n",
    "        email_dict[key] = val\n",
    "\n",
    "    # this step saves the body of the text; we apply some regex logic\n",
    "    text_body = temp_str[17:]\n",
    "    text_body = \" \".join([text for text in text_body]).strip()  # joins back all elements into a single string\n",
    "\n",
    "    # apply regex logic\n",
    "    text_body = re.sub(clean_field_headers, \"\", text_body)\n",
    "    text_body = re.sub(clean_html_tags, \"\", text_body)\n",
    "    text_body = re.sub(clean_emails, \"\", text_body)\n",
    "    text_body = re.sub(clean_fwds, \" \", text_body)\n",
    "    text_body = re.sub(clean_unintended_sends, \" \", text_body)\n",
    "    text_body = re.sub(clean_dashes, \" \", text_body)\n",
    "    text_body = re.sub(clean_transmission_warn, \" \", text_body)\n",
    "    text_body = re.sub(clean_datetime, \" \", text_body)\n",
    "    text_body = re.sub(clean_link, \" \", text_body)\n",
    "    text_body = re.sub(clean_phone_fax, \" \", text_body)\n",
    "    text_body = re.sub(clean_phone_ctrycode, \" \", text_body)\n",
    "    text_body = re.sub(clean_addr_code, \" \", text_body)\n",
    "    text_body = re.sub(clean_email_codes, \"\", text_body)\n",
    "    text_body = re.sub(clean_very_long_text, \"\", text_body)\n",
    "    text_body = re.sub(clean_multi_symbols, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_space, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_symbols, \" \", text_body)\n",
    "    text_body = re.sub(clean_multi_space, \" \", text_body)\n",
    "\n",
    "    # this step saves the email body text as a value to the key named \"body\"\n",
    "    email_dict[\"body\"] = text_body.strip()\n",
    "\n",
    "    # append the dictionary to a list, and later store as a dataframe\n",
    "    list_of_dict.append(email_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0965d",
   "metadata": {},
   "source": [
    "We compile the dictionary into a dataframe and renamed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "370bfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df_feat = pd.DataFrame(list_of_dict)\n",
    "emails_df_feat.columns = ['Message-ID', 'DateTime', 'From', 'To', 'Subject', 'Cc', 'Mime-Version',  # Date -> DateTime\n",
    "                          'Content-Type', 'Content-Transfer-Encoding', 'Bcc', 'X-From', 'X-To',\n",
    "                          'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName', 'body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639650c",
   "metadata": {},
   "source": [
    "We further clean up the email addresses in the \"From\". \"To\", \"Cc\", \"Bcc\" fields using Regex groups. E.g \"houston <.ward@enron.com>\", \"e-mail <.brandon@enron.com>\"; unlike the usual \"houston.ward@enron.com\".\n",
    "\n",
    "We then replace the columns with the cleaned up emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db4528b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - we do a clean up of email addresses\n",
    "reformat_emails = re.compile(r\"(?P<part1>[\\w-]+)[<\\s]*(?P<part2>[\\w.\\'\\W]+)(?P<domain>[@\\w.-]+)\")\n",
    "\n",
    "cleaned_from_emails = reformat_email_func(emails_df_feat, \"From\", reformat_emails)\n",
    "cleaned_to_emails = reformat_email_func(emails_df_feat, \"To\", reformat_emails)\n",
    "cleaned_cc_emails = reformat_email_func(emails_df_feat, \"Cc\", reformat_emails)\n",
    "cleaned_bcc_emails = reformat_email_func(emails_df_feat, \"Bcc\", reformat_emails)\n",
    "\n",
    "# - replace the columns with the cleaned up emails\n",
    "emails_df_feat.From = cleaned_from_emails\n",
    "emails_df_feat.To = cleaned_to_emails\n",
    "emails_df_feat.Cc = cleaned_cc_emails\n",
    "emails_df_feat.Bcc = cleaned_bcc_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fff7fe",
   "metadata": {},
   "source": [
    "We create new columns to include reformatted data: date, time, domain name (From and To) for emails.\n",
    "\n",
    "It seems like \"strptime\" cannot handle Timezone codes directly and more steps are required. We are not going to need Timezone information for our purpose, so we are going to ignore them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "199d0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_df_feat[\"date\"] = emails_df_feat[\"DateTime\"].apply(lambda x: datetime\n",
    "                                                          .strptime(x[:-6], \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "                                                          .strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "emails_df_feat[\"time\"] = emails_df_feat[\"DateTime\"].apply(lambda x: datetime\n",
    "                                                          .strptime(x[:-6], \"%a, %d %b %Y %H:%M:%S %z\")\n",
    "                                                          .strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "emails_df_feat[\"From_domain\"] = extract_domain(emails_df_feat, \"From\")\n",
    "emails_df_feat[\"To_domain\"] = extract_domain(emails_df_feat, \"To\")\n",
    "emails_df_feat[\"Cc_domain\"] = extract_domain(emails_df_feat, \"Cc\")\n",
    "emails_df_feat[\"Bcc_domain\"] = extract_domain(emails_df_feat, \"Bcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a398d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccff3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab9e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978263a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe51ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b257539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb020e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26d4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6510f8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8747d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff018e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272a50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec547f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ebe88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a5ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93b69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375f45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be95c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3319a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df776c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced26bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85411b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd26e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6224603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32710c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f61944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942d07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c09cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d0ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a5835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
